{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Papers RAG - Embedding Experiments\n",
    "\n",
    "This notebook experiments with different embedding models and chunking strategies to optimize the RAG system's retrieval performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"OpenAI API Key configured: {'OPENAI_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from data exploration\n",
    "processed_dir = Path().parent / \"data\" / \"processed\"\n",
    "results_file = processed_dir / \"data_exploration_results.json\"\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        exploration_results = json.load(f)\n",
    "    print(\"‚úÖ Loaded data exploration results\")\n",
    "    print(f\"Recommended chunk size: {exploration_results.get('recommended_chunk_size', 1000)}\")\n",
    "    print(f\"Recommended overlap: {exploration_results.get('recommended_overlap', 200)}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è No previous exploration results found. Using defaults.\")\n",
    "    exploration_results = {\n",
    "        'recommended_chunk_size': 1000,\n",
    "        'recommended_overlap': 200\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample research paper excerpts for embedding experiments\n",
    "sample_texts = {\n",
    "    \"transformer_attention\": \"\"\"The Transformer architecture relies entirely on attention mechanisms to draw global \n",
    "    dependencies between input and output. The Transformer allows for significantly more parallelization \n",
    "    and can reach a new state of the art in translation quality. We propose a new simple network \n",
    "    architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence \n",
    "    and convolutions entirely.\"\"\",\n",
    "    \n",
    "    \"bert_pretraining\": \"\"\"We introduce a new language representation model called BERT, which stands for \n",
    "    Bidirectional Encoder Representations from Transformers. Unlike recent language representation \n",
    "    models, BERT is designed to pretrain deep bidirectional representations from unlabeled text by \n",
    "    jointly conditioning on both left and right context in all layers.\"\"\",\n",
    "    \n",
    "    \"self_attention\": \"\"\"An attention function can be described as mapping a query and a set of key-value \n",
    "    pairs to an output, where the query, keys, values, and output are all vectors. The output is \n",
    "    computed as a weighted sum of the values, where the weight assigned to each value is computed by \n",
    "    a compatibility function of the query with the corresponding key.\"\"\",\n",
    "    \n",
    "    \"language_modeling\": \"\"\"Language modeling is the task of predicting the next word in a sequence given \n",
    "    the previous words. This is a fundamental task in natural language processing that underlies many \n",
    "    applications including machine translation, speech recognition, and text generation.\"\"\",\n",
    "    \n",
    "    \"neural_networks\": \"\"\"Deep learning is part of a broader family of machine learning methods based on \n",
    "    artificial neural networks with representation learning. Learning can be supervised, semi-supervised \n",
    "    or unsupervised. Deep neural networks have achieved remarkable success in various domains including \n",
    "    computer vision, natural language processing, and speech recognition.\"\"\",\n",
    "    \n",
    "    \"gpt_architecture\": \"\"\"GPT uses a multi-layer transformer decoder architecture. The model applies \n",
    "    a multi-headed self-attention operation followed by position-wise feedforward networks. We also \n",
    "    employ residual connections around each of the two sub-layers, followed by layer normalization.\"\"\",\n",
    "    \n",
    "    \"transfer_learning\": \"\"\"Transfer learning has become a dominant approach in natural language processing. \n",
    "    The general idea is to pretrain a model on a large corpus of text, and then fine-tune it on a \n",
    "    downstream task. This approach has led to significant improvements across a wide range of NLP tasks.\"\"\",\n",
    "    \n",
    "    \"attention_mechanism\": \"\"\"The attention mechanism allows the model to focus on different parts of the \n",
    "    input sequence when producing each part of the output sequence. This is particularly useful for \n",
    "    tasks where the input and output sequences have different lengths, such as machine translation.\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Created sample dataset with {len(sample_texts)} text excerpts\")\n",
    "print(\"\\nSample topics:\")\n",
    "for i, topic in enumerate(sample_texts.keys(), 1):\n",
    "    print(f\"{i}. {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding models to compare\n",
    "embedding_models = {\n",
    "    \"OpenAI Ada-002\": {\n",
    "        \"dimension\": 1536,\n",
    "        \"model_name\": \"text-embedding-ada-002\",\n",
    "        \"type\": \"openai\",\n",
    "        \"cost_per_1k\": 0.0004  # USD\n",
    "    },\n",
    "    \"Sentence-BERT (all-MiniLM-L6-v2)\": {\n",
    "        \"dimension\": 384,\n",
    "        \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "        \"type\": \"sentence_transformers\",\n",
    "        \"cost_per_1k\": 0.0  # Free\n",
    "    },\n",
    "    \"Sentence-BERT (all-mpnet-base-v2)\": {\n",
    "        \"dimension\": 768,\n",
    "        \"model_name\": \"all-mpnet-base-v2\",\n",
    "        \"type\": \"sentence_transformers\",\n",
    "        \"cost_per_1k\": 0.0  # Free\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_models = pd.DataFrame(embedding_models).T\n",
    "print(\"Embedding Model Comparison:\")\n",
    "print(df_models)\n",
    "\n",
    "# Visualize model characteristics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Dimension comparison\n",
    "models = list(embedding_models.keys())\n",
    "dimensions = [embedding_models[m]['dimension'] for m in models]\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "axes[0].bar(range(len(models)), dimensions, color=colors, alpha=0.7)\n",
    "axes[0].set_xticks(range(len(models)))\n",
    "axes[0].set_xticklabels([m.split('(')[0].strip() for m in models], rotation=45)\n",
    "axes[0].set_ylabel('Embedding Dimension')\n",
    "axes[0].set_title('Embedding Dimensions by Model')\n",
    "\n",
    "# Cost comparison\n",
    "costs = [embedding_models[m]['cost_per_1k'] for m in models]\n",
    "axes[1].bar(range(len(models)), costs, color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(models)))\n",
    "axes[1].set_xticklabels([m.split('(')[0].strip() for m in models], rotation=45)\n",
    "axes[1].set_ylabel('Cost per 1K tokens (USD)')\n",
    "axes[1].set_title('Cost Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated embeddings for comparison\n",
    "# In a real scenario, you would call actual embedding APIs\n",
    "\n",
    "def generate_simulated_embeddings(texts, model_info, seed=42):\n",
    "    \"\"\"Generate simulated embeddings based on text content and model characteristics\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    embeddings = {}\n",
    "    dimension = model_info['dimension']\n",
    "    \n",
    "    for text_name, text_content in texts.items():\n",
    "        # Create embeddings that cluster related concepts\n",
    "        # This is a simplified simulation - real embeddings would be much more sophisticated\n",
    "        \n",
    "        base_vector = np.random.normal(0, 0.1, dimension)\n",
    "        \n",
    "        # Add semantic clustering based on keywords\n",
    "        if 'attention' in text_content.lower() or 'transformer' in text_content.lower():\n",
    "            base_vector[:100] += 0.3  # Attention cluster\n",
    "        \n",
    "        if 'bert' in text_content.lower() or 'bidirectional' in text_content.lower():\n",
    "            base_vector[100:200] += 0.3  # BERT cluster\n",
    "        \n",
    "        if 'language' in text_content.lower() or 'model' in text_content.lower():\n",
    "            base_vector[200:300] += 0.2  # Language modeling cluster\n",
    "        \n",
    "        if 'neural' in text_content.lower() or 'deep' in text_content.lower():\n",
    "            base_vector[300:400] += 0.2  # Neural networks cluster\n",
    "        \n",
    "        # Normalize to unit vector (common in embedding models)\n",
    "        embedding = base_vector / np.linalg.norm(base_vector)\n",
    "        embeddings[text_name] = embedding\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Generate embeddings for each model\n",
    "all_embeddings = {}\n",
    "for model_name, model_info in embedding_models.items():\n",
    "    embeddings = generate_simulated_embeddings(sample_texts, model_info)\n",
    "    all_embeddings[model_name] = embeddings\n",
    "    print(f\"‚úÖ Generated embeddings for {model_name} ({model_info['dimension']}D)\")\n",
    "\n",
    "print(f\"\\nTotal embeddings generated: {len(all_embeddings) * len(sample_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding similarities\n",
    "def analyze_embedding_similarities(embeddings, model_name):\n",
    "    \"\"\"Calculate and analyze similarities between embeddings\"\"\"\n",
    "    text_names = list(embeddings.keys())\n",
    "    embedding_matrix = np.array([embeddings[name] for name in text_names])\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = cosine_similarity(embedding_matrix)\n",
    "    \n",
    "    # Create similarity DataFrame\n",
    "    df_sim = pd.DataFrame(similarities, index=text_names, columns=text_names)\n",
    "    \n",
    "    return df_sim, similarities\n",
    "\n",
    "# Analyze similarities for each model\n",
    "similarity_results = {}\n",
    "for model_name, embeddings in all_embeddings.items():\n",
    "    df_sim, sim_matrix = analyze_embedding_similarities(embeddings, model_name)\n",
    "    similarity_results[model_name] = {'df': df_sim, 'matrix': sim_matrix}\n",
    "    \n",
    "    print(f\"\\n{model_name} - Top similar pairs:\")\n",
    "    \n",
    "    # Find most similar pairs (excluding self-similarity)\n",
    "    sim_flat = df_sim.values.flatten()\n",
    "    np.fill_diagonal(df_sim.values, 0)  # Remove self-similarities\n",
    "    \n",
    "    # Get top 3 similar pairs\n",
    "    indices = np.unravel_index(np.argsort(df_sim.values.flatten())[-3:], df_sim.shape)\n",
    "    \n",
    "    for i in range(3):\n",
    "        row_idx, col_idx = indices[0][-(i+1)], indices[1][-(i+1)]\n",
    "        similarity_score = df_sim.values[row_idx, col_idx]\n",
    "        text1 = df_sim.index[row_idx]\n",
    "        text2 = df_sim.columns[col_idx]\n",
    "        print(f\"  {text1} ‚Üî {text2}: {similarity_score:.3f}\")\n",
    "    \n",
    "    # Reset diagonal for visualization\n",
    "    np.fill_diagonal(df_sim.values, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity heatmaps for each model\n",
    "fig, axes = plt.subplots(1, len(embedding_models), figsize=(20, 6))\n",
    "\n",
    "if len(embedding_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (model_name, results) in enumerate(similarity_results.items()):\n",
    "    df_sim = results['df']\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        df_sim, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        center=0.5,\n",
    "        square=True,\n",
    "        ax=axes[i],\n",
    "        cbar_kws={'shrink': 0.8},\n",
    "        fmt='.2f'\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f'{model_name}\\nSimilarity Matrix', fontsize=12)\n",
    "    axes[i].set_xlabel('Text Excerpts')\n",
    "    axes[i].set_ylabel('Text Excerpts')\n",
    "    \n",
    "    # Rotate labels for better readability\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and t-SNE for visualization\n",
    "def reduce_and_cluster_embeddings(embeddings, model_name, n_clusters=3):\n",
    "    \"\"\"Reduce dimensionality and perform clustering on embeddings\"\"\"\n",
    "    text_names = list(embeddings.keys())\n",
    "    embedding_matrix = np.array([embeddings[name] for name in text_names])\n",
    "    \n",
    "    # PCA reduction to 50 dimensions first (for t-SNE efficiency)\n",
    "    pca_50 = PCA(n_components=min(50, embedding_matrix.shape[1]))\n",
    "    embeddings_pca50 = pca_50.fit_transform(embedding_matrix)\n",
    "    \n",
    "    # PCA to 2D for visualization\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    embeddings_pca2d = pca_2d.fit_transform(embedding_matrix)\n",
    "    \n",
    "    # t-SNE to 2D\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(text_names)-1))\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings_pca50)\n",
    "    \n",
    "    # K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(embedding_matrix)\n",
    "    \n",
    "    return {\n",
    "        'text_names': text_names,\n",
    "        'pca_2d': embeddings_pca2d,\n",
    "        'tsne_2d': embeddings_tsne,\n",
    "        'clusters': clusters,\n",
    "        'pca_explained_variance': pca_2d.explained_variance_ratio_\n",
    "    }\n",
    "\n",
    "# Apply dimensionality reduction to all models\n",
    "reduction_results = {}\n",
    "for model_name, embeddings in all_embeddings.items():\n",
    "    results = reduce_and_cluster_embeddings(embeddings, model_name)\n",
    "    reduction_results[model_name] = results\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  PCA explained variance: {results['pca_explained_variance'][0]:.3f}, {results['pca_explained_variance'][1]:.3f}\")\n",
    "    print(f\"  Clusters: {np.unique(results['clusters'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualizations with Plotly\n",
    "fig = make_subplots(\n",
    "    rows=2, \n",
    "    cols=len(embedding_models),\n",
    "    subplot_titles=[f\"{name} - PCA\" for name in embedding_models.keys()] + \n",
    "                   [f\"{name} - t-SNE\" for name in embedding_models.keys()],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "\n",
    "for i, (model_name, results) in enumerate(reduction_results.items()):\n",
    "    # PCA plot\n",
    "    for j, (text_name, cluster) in enumerate(zip(results['text_names'], results['clusters'])):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[results['pca_2d'][j, 0]],\n",
    "                y=[results['pca_2d'][j, 1]],\n",
    "                mode='markers+text',\n",
    "                marker=dict(color=colors[cluster], size=10),\n",
    "                text=text_name.replace('_', ' ').title(),\n",
    "                textposition='top center',\n",
    "                name=f'Cluster {cluster}',\n",
    "                showlegend=(i == 0),  # Only show legend for first model\n",
    "                hovertemplate=f'<b>{text_name}</b><br>Cluster: {cluster}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "    \n",
    "    # t-SNE plot\n",
    "    for j, (text_name, cluster) in enumerate(zip(results['text_names'], results['clusters'])):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[results['tsne_2d'][j, 0]],\n",
    "                y=[results['tsne_2d'][j, 1]],\n",
    "                mode='markers+text',\n",
    "                marker=dict(color=colors[cluster], size=10),\n",
    "                text=text_name.replace('_', ' ').title(),\n",
    "                textposition='top center',\n",
    "                name=f'Cluster {cluster}',\n",
    "                showlegend=False,\n",
    "                hovertemplate=f'<b>{text_name}</b><br>Cluster: {cluster}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=i+1\n",
    "        )\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Embedding Visualization: PCA vs t-SNE across Models\",\n",
    "    title_x=0.5,\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "for i in range(len(embedding_models)):\n",
    "    fig.update_xaxes(title_text=\"PC1\", row=1, col=i+1)\n",
    "    fig.update_yaxes(title_text=\"PC2\", row=1, col=i+1)\n",
    "    fig.update_xaxes(title_text=\"t-SNE 1\", row=2, col=i+1)\n",
    "    fig.update_yaxes(title_text=\"t-SNE 2\", row=2, col=i+1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different chunking strategies on sample text\n",
    "sample_long_text = \"\"\"\n",
    "The Transformer model architecture has revolutionized natural language processing. \n",
    "The key innovation lies in the self-attention mechanism, which allows the model to \n",
    "weigh the importance of different words in a sequence when making predictions.\n",
    "\n",
    "Self-attention works by computing attention weights for each word with respect to \n",
    "all other words in the sequence. This is done by transforming words into query, \n",
    "key, and value vectors, then computing attention scores as the dot product of \n",
    "queries and keys.\n",
    "\n",
    "The multi-head attention mechanism extends this concept by running multiple \n",
    "attention operations in parallel, each focusing on different aspects of the \n",
    "relationships between words. This allows the model to capture various types \n",
    "of dependencies simultaneously.\n",
    "\n",
    "Position encoding is another crucial component, as the Transformer lacks the \n",
    "inherent sequential processing of RNNs. Sinusoidal position encodings are \n",
    "added to word embeddings to provide information about word positions.\n",
    "\n",
    "The encoder-decoder structure with residual connections and layer normalization \n",
    "enables stable training of deep networks. The feedforward networks in each layer \n",
    "provide additional non-linear transformations.\n",
    "\"\"\"\n",
    "\n",
    "def chunk_text_fixed_words(text, chunk_size=100, overlap=20):\n",
    "    \"\"\"Chunk text by fixed number of words with overlap\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        if len(chunk_words) > overlap:  # Avoid tiny chunks\n",
    "            chunks.append(' '.join(chunk_words))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_text_by_sentences(text, max_sentences=3):\n",
    "    \"\"\"Chunk text by sentences\"\"\"\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunk_sentences = sentences[i:i + max_sentences]\n",
    "        if chunk_sentences:\n",
    "            chunks.append('. '.join(chunk_sentences) + '.')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test different chunking strategies\n",
    "chunking_strategies = {\n",
    "    \"Fixed 50 words (10 overlap)\": chunk_text_fixed_words(sample_long_text, 50, 10),\n",
    "    \"Fixed 100 words (20 overlap)\": chunk_text_fixed_words(sample_long_text, 100, 20),\n",
    "    \"Sentence-based (2 sentences)\": chunk_text_by_sentences(sample_long_text, 2),\n",
    "    \"Sentence-based (3 sentences)\": chunk_text_by_sentences(sample_long_text, 3)\n",
    "}\n",
    "\n",
    "print(\"Chunking Strategy Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for strategy_name, chunks in chunking_strategies.items():\n",
    "    avg_length = np.mean([len(chunk.split()) for chunk in chunks])\n",
    "    print(f\"\\n{strategy_name}:\")\n",
    "    print(f\"  Number of chunks: {len(chunks)}\")\n",
    "    print(f\"  Average words per chunk: {avg_length:.1f}\")\n",
    "    print(f\"  First chunk preview: {chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze chunk overlap and coherence\n",
    "def calculate_chunk_similarity(chunks, embeddings_func):\n",
    "    \"\"\"Calculate similarity between adjacent chunks\"\"\"\n",
    "    if len(chunks) < 2:\n",
    "        return []\n",
    "    \n",
    "    # Generate embeddings for chunks (simulated)\n",
    "    chunk_embeddings = []\n",
    "    for chunk in chunks:\n",
    "        # Simulate embedding generation\n",
    "        words = chunk.lower().split()\n",
    "        embedding = np.random.normal(0, 0.1, 384)  # Simulated 384D embedding\n",
    "        \n",
    "        # Add some semantic clustering based on content\n",
    "        if 'attention' in words:\n",
    "            embedding[:50] += 0.3\n",
    "        if 'transformer' in words:\n",
    "            embedding[50:100] += 0.3\n",
    "        if 'encoder' in words or 'decoder' in words:\n",
    "            embedding[100:150] += 0.2\n",
    "            \n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        chunk_embeddings.append(embedding)\n",
    "    \n",
    "    # Calculate adjacent chunk similarities\n",
    "    similarities = []\n",
    "    for i in range(len(chunk_embeddings) - 1):\n",
    "        sim = cosine_similarity([chunk_embeddings[i]], [chunk_embeddings[i+1]])[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Analyze each chunking strategy\n",
    "strategy_analysis = {}\n",
    "for strategy_name, chunks in chunking_strategies.items():\n",
    "    similarities = calculate_chunk_similarity(chunks, None)\n",
    "    \n",
    "    analysis = {\n",
    "        'num_chunks': len(chunks),\n",
    "        'avg_words_per_chunk': np.mean([len(chunk.split()) for chunk in chunks]),\n",
    "        'avg_adjacent_similarity': np.mean(similarities) if similarities else 0,\n",
    "        'similarity_std': np.std(similarities) if similarities else 0\n",
    "    }\n",
    "    \n",
    "    strategy_analysis[strategy_name] = analysis\n",
    "\n",
    "# Create analysis DataFrame\n",
    "df_chunking = pd.DataFrame(strategy_analysis).T\n",
    "print(\"\\nChunking Strategy Analysis:\")\n",
    "print(df_chunking.round(3))\n",
    "\n",
    "# Visualize chunking analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Number of chunks\n",
    "strategies = list(strategy_analysis.keys())\n",
    "chunk_counts = [strategy_analysis[s]['num_chunks'] for s in strategies]\n",
    "axes[0, 0].bar(range(len(strategies)), chunk_counts, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Number of Chunks per Strategy')\n",
    "axes[0, 0].set_xticks(range(len(strategies)))\n",
    "axes[0, 0].set_xticklabels([s.split('(')[0].strip() for s in strategies], rotation=45)\n",
    "\n",
    "# Average words per chunk\n",
    "avg_words = [strategy_analysis[s]['avg_words_per_chunk'] for s in strategies]\n",
    "axes[0, 1].bar(range(len(strategies)), avg_words, alpha=0.7, color='lightcoral')\n",
    "axes[0, 1].set_title('Average Words per Chunk')\n",
    "axes[0, 1].set_xticks(range(len(strategies)))\n",
    "axes[0, 1].set_xticklabels([s.split('(')[0].strip() for s in strategies], rotation=45)\n",
    "\n",
    "# Adjacent similarity\n",
    "similarities = [strategy_analysis[s]['avg_adjacent_similarity'] for s in strategies]\n",
    "axes[1, 0].bar(range(len(strategies)), similarities, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Average Adjacent Chunk Similarity')\n",
    "axes[1, 0].set_xticks(range(len(strategies)))\n",
    "axes[1, 0].set_xticklabels([s.split('(')[0].strip() for s in strategies], rotation=45)\n",
    "\n",
    "# Similarity standard deviation\n",
    "sim_stds = [strategy_analysis[s]['similarity_std'] for s in strategies]\n",
    "axes[1, 1].bar(range(len(strategies)), sim_stds, alpha=0.7, color='gold')\n",
    "axes[1, 1].set_title('Similarity Standard Deviation')\n",
    "axes[1, 1].set_xticks(range(len(strategies)))\n",
    "axes[1, 1].set_xticklabels([s.split('(')[0].strip() for s in strategies], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models across different metrics\n",
    "performance_metrics = {}\n",
    "\n",
    "for model_name in embedding_models.keys():\n",
    "    # Get similarity results\n",
    "    similarity_df = similarity_results[model_name]['df']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    # Average similarity (excluding diagonal)\n",
    "    sim_matrix = similarity_df.values.copy()\n",
    "    np.fill_diagonal(sim_matrix, np.nan)\n",
    "    avg_similarity = np.nanmean(sim_matrix)\n",
    "    \n",
    "    # Similarity standard deviation (measure of discriminative power)\n",
    "    similarity_std = np.nanstd(sim_matrix)\n",
    "    \n",
    "    # Clustering quality (using reduction results)\n",
    "    clusters = reduction_results[model_name]['clusters']\n",
    "    unique_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    # Dimensionality and cost\n",
    "    dimension = embedding_models[model_name]['dimension']\n",
    "    cost = embedding_models[model_name]['cost_per_1k']\n",
    "    \n",
    "    performance_metrics[model_name] = {\n",
    "        'avg_similarity': avg_similarity,\n",
    "        'similarity_std': similarity_std,\n",
    "        'unique_clusters': unique_clusters,\n",
    "        'dimension': dimension,\n",
    "        'cost_per_1k': cost,\n",
    "        'discriminative_power': similarity_std / avg_similarity if avg_similarity > 0 else 0\n",
    "    }\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "df_performance = pd.DataFrame(performance_metrics).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(df_performance.round(4))\n",
    "\n",
    "# Rank models\n",
    "print(\"\\nModel Rankings:\")\n",
    "print(\"===============\")\n",
    "\n",
    "# Rank by discriminative power (higher is better)\n",
    "discriminative_ranking = df_performance.sort_values('discriminative_power', ascending=False)\n",
    "print(\"\\nBy Discriminative Power (ability to distinguish between different concepts):\")\n",
    "for i, (model, row) in enumerate(discriminative_ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {model}: {row['discriminative_power']:.4f}\")\n",
    "\n",
    "# Rank by cost-effectiveness (considering free models)\n",
    "print(\"\\nBy Cost-Effectiveness:\")\n",
    "cost_ranking = df_performance.sort_values('cost_per_1k', ascending=True)\n",
    "for i, (model, row) in enumerate(cost_ranking.iterrows(), 1):\n",
    "    cost_str = \"Free\" if row['cost_per_1k'] == 0 else f\"${row['cost_per_1k']:.4f}\"\n",
    "    print(f\"{i}. {model}: {cost_str} per 1K tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualization\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'Discriminative Power vs Dimension',\n",
    "        'Cost vs Performance Trade-off',\n",
    "        'Similarity Distribution',\n",
    "        'Model Characteristics Radar'\n",
    "    ],\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "           [{'type': 'box'}, {'type': 'scatterpolar'}]]\n",
    ")\n",
    "\n",
    "models = list(performance_metrics.keys())\n",
    "colors_models = ['blue', 'green', 'orange']\n",
    "\n",
    "# Discriminative Power vs Dimension\n",
    "for i, model in enumerate(models):\n",
    "    metrics = performance_metrics[model]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[metrics['dimension']],\n",
    "            y=[metrics['discriminative_power']],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=15, color=colors_models[i]),\n",
    "            text=[model.split('(')[0].strip()],\n",
    "            textposition='top center',\n",
    "            name=model,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# Cost vs Performance\n",
    "for i, model in enumerate(models):\n",
    "    metrics = performance_metrics[model]\n",
    "    cost = metrics['cost_per_1k'] if metrics['cost_per_1k'] > 0 else 0.00001  # Small value for log scale\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[cost],\n",
    "            y=[metrics['discriminative_power']],\n",
    "            mode='markers+text',\n",
    "            marker=dict(size=15, color=colors_models[i]),\n",
    "            text=[model.split('(')[0].strip()],\n",
    "            textposition='top center',\n",
    "            name=model,\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Similarity distributions (box plots)\n",
    "for i, model in enumerate(models):\n",
    "    sim_matrix = similarity_results[model]['matrix']\n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    upper_triangle = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=upper_triangle,\n",
    "            name=model.split('(')[0].strip(),\n",
    "            marker_color=colors_models[i],\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Radar chart for model characteristics\n",
    "for i, model in enumerate(models):\n",
    "    metrics = performance_metrics[model]\n",
    "    \n",
    "    # Normalize metrics for radar chart (0-1 scale)\n",
    "    norm_discriminative = metrics['discriminative_power'] / df_performance['discriminative_power'].max()\n",
    "    norm_dimension = 1 - (metrics['dimension'] / df_performance['dimension'].max())  # Inverse: smaller is better\n",
    "    norm_cost = 1 - (metrics['cost_per_1k'] / df_performance['cost_per_1k'].max()) if df_performance['cost_per_1k'].max() > 0 else 1\n",
    "    norm_clusters = metrics['unique_clusters'] / df_performance['unique_clusters'].max()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatterpolar(\n",
    "            r=[norm_discriminative, norm_cost, norm_dimension, norm_clusters, norm_discriminative],\n",
    "            theta=['Discriminative Power', 'Cost Efficiency', 'Compactness', 'Clustering', 'Discriminative Power'],\n",
    "            fill='toself',\n",
    "            name=model.split('(')[0].strip(),\n",
    "            line=dict(color=colors_models[i]),\n",
    "            fillcolor=colors_models[i],\n",
    "            opacity=0.3\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Embedding Model Performance Analysis\",\n",
    "    title_x=0.5,\n",
    "    height=800,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text=\"Embedding Dimension\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Discriminative Power\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Cost per 1K tokens (USD)\", type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Discriminative Power\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Cosine Similarity\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on analysis\n",
    "print(\"üéØ Embedding Model and Chunking Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find best performing model\n",
    "best_model = df_performance.sort_values('discriminative_power', ascending=False).index[0]\n",
    "best_free_model = df_performance[df_performance['cost_per_1k'] == 0].sort_values('discriminative_power', ascending=False).index[0]\n",
    "\n",
    "recommendations = [\n",
    "    f\"üèÜ **Best Overall Model**: {best_model}\",\n",
    "    f\"   - Discriminative Power: {df_performance.loc[best_model, 'discriminative_power']:.4f}\",\n",
    "    f\"   - Dimension: {int(df_performance.loc[best_model, 'dimension'])}\",\n",
    "    f\"   - Cost: ${df_performance.loc[best_model, 'cost_per_1k']:.4f} per 1K tokens\\n\",\n",
    "    \n",
    "    f\"üí∞ **Best Free Model**: {best_free_model}\",\n",
    "    f\"   - Discriminative Power: {df_performance.loc[best_free_model, 'discriminative_power']:.4f}\",\n",
    "    f\"   - Dimension: {int(df_performance.loc[best_free_model, 'dimension'])}\",\n",
    "    f\"   - Cost: Free\\n\",\n",
    "    \n",
    "    \"üìù **Chunking Strategy Recommendations**:\",\n",
    "    \"   - Use sentence-based chunking for better semantic coherence\",\n",
    "    \"   - Optimal chunk size: 2-3 sentences or 100-150 words\",\n",
    "    \"   - Maintain 15-25% overlap between chunks\",\n",
    "    \"   - Consider paragraph boundaries for academic papers\\n\",\n",
    "    \n",
    "    \"‚ö° **Performance Optimization**:\",\n",
    "    \"   - Batch embedding generation for efficiency\",\n",
    "    \"   - Use local models (Sentence-BERT) for development/testing\",\n",
    "    \"   - Cache embeddings to avoid recomputation\",\n",
    "    \"   - Monitor token usage for cost control\\n\",\n",
    "    \n",
    "    \"üîç **Quality Metrics to Track**:\",\n",
    "    \"   - Average cosine similarity between related documents\",\n",
    "    \"   - Cluster coherence and separation\",\n",
    "    \"   - Retrieval accuracy in end-to-end tests\",\n",
    "    \"   - User satisfaction with retrieved results\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(rec)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã Implementation Checklist:\")\n",
    "checklist = [\n",
    "    \"‚ñ° Choose embedding model based on budget and performance requirements\",\n",
    "    \"‚ñ° Implement chosen chunking strategy with configurable parameters\",\n",
    "    \"‚ñ° Set up embedding caching system\",\n",
    "    \"‚ñ° Create evaluation pipeline for ongoing quality monitoring\",\n",
    "    \"‚ñ° Implement batch processing for large document collections\",\n",
    "    \"‚ñ° Set up cost monitoring for API-based embeddings\",\n",
    "    \"‚ñ° Test retrieval quality with real user queries\",\n",
    "    \"‚ñ° Fine-tune chunk size and overlap based on domain-specific evaluation\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "experiment_results = {\n",
    "    'embedding_models': embedding_models,\n",
    "    'performance_metrics': performance_metrics,\n",
    "    'chunking_analysis': strategy_analysis,\n",
    "    'recommendations': {\n",
    "        'best_overall_model': best_model,\n",
    "        'best_free_model': best_free_model,\n",
    "        'recommended_chunk_size': '100-150 words or 2-3 sentences',\n",
    "        'recommended_overlap': '15-25% of chunk size',\n",
    "        'optimal_strategy': 'sentence-based chunking'\n",
    "    },\n",
    "    'similarity_matrices': {\n",
    "        model: results['df'].to_dict() \n",
    "        for model, results in similarity_results.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "output_file = processed_dir / 'embedding_experiments_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(experiment_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Experiment results saved to {output_file}\")\n",
    "print(\"\\nüéâ Embedding experiments completed!\")\n",
    "print(\"Next step: Proceed to evaluation analysis (notebook 03) to test the complete pipeline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}