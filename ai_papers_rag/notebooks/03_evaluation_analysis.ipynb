{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Papers RAG - Evaluation Analysis\n",
    "\n",
    "This notebook analyzes the performance of the complete RAG system, evaluates retrieval quality, and provides insights for system optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Analysis timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from previous notebooks\n",
    "processed_dir = Path().parent / \"data\" / \"processed\"\n",
    "\n",
    "# Load data exploration results\n",
    "data_exploration_file = processed_dir / \"data_exploration_results.json\"\n",
    "embedding_experiments_file = processed_dir / \"embedding_experiments_results.json\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "if data_exploration_file.exists():\n",
    "    with open(data_exploration_file, 'r') as f:\n",
    "        results['data_exploration'] = json.load(f)\n",
    "    print(\"✅ Loaded data exploration results\")\n",
    "else:\n",
    "    print(\"⚠️ Data exploration results not found\")\n",
    "\n",
    "if embedding_experiments_file.exists():\n",
    "    with open(embedding_experiments_file, 'r') as f:\n",
    "        results['embedding_experiments'] = json.load(f)\n",
    "    print(\"✅ Loaded embedding experiments results\")\n",
    "else:\n",
    "    print(\"⚠️ Embedding experiments results not found\")\n",
    "\n",
    "print(f\"\\nLoaded {len(results)} result files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Simulated RAG Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "evaluation_queries = [\n",
    "    {\n",
    "        \"query_id\": \"q001\",\n",
    "        \"query\": \"What is the transformer architecture?\",\n",
    "        \"category\": \"architecture\",\n",
    "        \"difficulty\": \"basic\",\n",
    "        \"expected_papers\": [\"attention_is_all_you_need\", \"transformer_tutorial\"],\n",
    "        \"ground_truth_answer\": \"The transformer is a neural network architecture based solely on attention mechanisms.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q002\",\n",
    "        \"query\": \"How does BERT differ from GPT in training methodology?\",\n",
    "        \"category\": \"comparison\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"expected_papers\": [\"bert_paper\", \"gpt_paper\"],\n",
    "        \"ground_truth_answer\": \"BERT uses bidirectional training while GPT uses autoregressive training.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q003\",\n",
    "        \"query\": \"What are the computational complexity implications of self-attention?\",\n",
    "        \"category\": \"technical\",\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"expected_papers\": [\"attention_analysis\", \"efficiency_study\"],\n",
    "        \"ground_truth_answer\": \"Self-attention has quadratic complexity with respect to sequence length.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q004\",\n",
    "        \"query\": \"What are the main applications of large language models?\",\n",
    "        \"category\": \"applications\",\n",
    "        \"difficulty\": \"basic\",\n",
    "        \"expected_papers\": [\"llm_applications\", \"gpt3_paper\"],\n",
    "        \"ground_truth_answer\": \"LLMs are used for text generation, translation, summarization, and question answering.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q005\",\n",
    "        \"query\": \"How do positional encodings work in transformers?\",\n",
    "        \"category\": \"technical\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"expected_papers\": [\"attention_is_all_you_need\", \"positional_encoding_study\"],\n",
    "        \"ground_truth_answer\": \"Positional encodings provide sequence order information using sinusoidal functions.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q006\",\n",
    "        \"query\": \"What is masked language modeling?\",\n",
    "        \"category\": \"concepts\",\n",
    "        \"difficulty\": \"basic\",\n",
    "        \"expected_papers\": [\"bert_paper\", \"masked_lm_study\"],\n",
    "        \"ground_truth_answer\": \"Masked language modeling predicts masked tokens using bidirectional context.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q007\",\n",
    "        \"query\": \"How does multi-head attention improve model performance?\",\n",
    "        \"category\": \"mechanisms\",\n",
    "        \"difficulty\": \"intermediate\",\n",
    "        \"expected_papers\": [\"attention_is_all_you_need\", \"multihead_analysis\"],\n",
    "        \"ground_truth_answer\": \"Multi-head attention allows the model to attend to different representation subspaces.\"\n",
    "    },\n",
    "    {\n",
    "        \"query_id\": \"q008\",\n",
    "        \"query\": \"What are the scaling laws for transformer models?\",\n",
    "        \"category\": \"scaling\",\n",
    "        \"difficulty\": \"advanced\",\n",
    "        \"expected_papers\": [\"scaling_laws_paper\", \"gpt3_paper\"],\n",
    "        \"ground_truth_answer\": \"Model performance scales predictably with size, data, and compute resources.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(evaluation_queries)} queries\")\n",
    "print(\"\\nQuery distribution:\")\n",
    "df_queries = pd.DataFrame(evaluation_queries)\n",
    "print(df_queries.groupby(['category', 'difficulty']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate RAG System Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate RAG system responses with realistic performance characteristics\n",
    "def simulate_rag_response(query_data, model_quality=0.8):\n",
    "    \"\"\"Simulate a RAG system response with realistic metrics\"\"\"\n",
    "    \n",
    "    # Response time varies by query complexity\n",
    "    base_time = 1.5  # seconds\n",
    "    difficulty_multiplier = {\n",
    "        'basic': 1.0,\n",
    "        'intermediate': 1.3,\n",
    "        'advanced': 1.8\n",
    "    }\n",
    "    \n",
    "    response_time = base_time * difficulty_multiplier[query_data['difficulty']] + np.random.normal(0, 0.3)\n",
    "    response_time = max(0.5, response_time)  # Minimum 0.5 seconds\n",
    "    \n",
    "    # Number of retrieved documents\n",
    "    num_retrieved = np.random.randint(3, 8)\n",
    "    \n",
    "    # Simulate retrieved documents with scores\n",
    "    retrieved_docs = []\n",
    "    for i in range(num_retrieved):\n",
    "        # Higher scores for expected papers\n",
    "        is_relevant = i < len(query_data['expected_papers']) and np.random.random() > 0.2\n",
    "        \n",
    "        if is_relevant:\n",
    "            score = np.random.beta(8, 2)  # Skewed towards high scores\n",
    "            paper_id = query_data['expected_papers'][i % len(query_data['expected_papers'])]\n",
    "        else:\n",
    "            score = np.random.beta(2, 5)  # Skewed towards low scores\n",
    "            paper_id = f\"unrelated_paper_{i}\"\n",
    "        \n",
    "        retrieved_docs.append({\n",
    "            'paper_id': paper_id,\n",
    "            'score': score,\n",
    "            'relevant': is_relevant\n",
    "        })\n",
    "    \n",
    "    # Sort by score\n",
    "    retrieved_docs.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Generate answer quality metrics\n",
    "    # Quality depends on retrieved document relevance and model capability\n",
    "    relevant_docs_score = np.mean([doc['score'] for doc in retrieved_docs if doc['relevant']])\n",
    "    if np.isnan(relevant_docs_score):\n",
    "        relevant_docs_score = 0.3\n",
    "    \n",
    "    answer_quality = model_quality * relevant_docs_score * np.random.uniform(0.8, 1.2)\n",
    "    answer_quality = np.clip(answer_quality, 0, 1)\n",
    "    \n",
    "    # Calculate retrieval metrics\n",
    "    relevant_retrieved = sum(1 for doc in retrieved_docs if doc['relevant'])\n",
    "    total_relevant = len(query_data['expected_papers'])\n",
    "    \n",
    "    precision = relevant_retrieved / num_retrieved if num_retrieved > 0 else 0\n",
    "    recall = relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Simulate generated answer\n",
    "    answer_length = np.random.randint(50, 300)  # words\n",
    "    \n",
    "    # Citation quality\n",
    "    citation_quality = min(1.0, relevant_retrieved * 0.3)\n",
    "    \n",
    "    return {\n",
    "        'query_id': query_data['query_id'],\n",
    "        'response_time': response_time,\n",
    "        'num_retrieved': num_retrieved,\n",
    "        'relevant_retrieved': relevant_retrieved,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'answer_quality': answer_quality,\n",
    "        'answer_length': answer_length,\n",
    "        'citation_quality': citation_quality,\n",
    "        'retrieved_docs': retrieved_docs,\n",
    "        'top_score': max(doc['score'] for doc in retrieved_docs) if retrieved_docs else 0\n",
    "    }\n",
    "\n",
    "# Simulate responses for different model configurations\n",
    "model_configurations = {\n",
    "    'OpenAI + GPT-3.5': {'retrieval_quality': 0.85, 'generation_quality': 0.9},\n",
    "    'SentenceBERT + GPT-3.5': {'retrieval_quality': 0.75, 'generation_quality': 0.9},\n",
    "    'SentenceBERT + Local LLM': {'retrieval_quality': 0.75, 'generation_quality': 0.7}\n",
    "}\n",
    "\n",
    "all_responses = {}\n",
    "\n",
    "for config_name, config_params in model_configurations.items():\n",
    "    responses = []\n",
    "    for query in evaluation_queries:\n",
    "        # Combine retrieval and generation quality\n",
    "        overall_quality = (config_params['retrieval_quality'] + config_params['generation_quality']) / 2\n",
    "        response = simulate_rag_response(query, overall_quality)\n",
    "        responses.append(response)\n",
    "    \n",
    "    all_responses[config_name] = responses\n",
    "    print(f\"✅ Generated {len(responses)} responses for {config_name}\")\n",
    "\n",
    "print(f\"\\nTotal simulated responses: {sum(len(responses) for responses in all_responses.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance metrics across configurations\n",
    "def analyze_configuration_performance(responses):\n",
    "    \"\"\"Analyze performance metrics for a configuration\"\"\"\n",
    "    df = pd.DataFrame(responses)\n",
    "    \n",
    "    return {\n",
    "        'avg_response_time': df['response_time'].mean(),\n",
    "        'avg_precision': df['precision'].mean(),\n",
    "        'avg_recall': df['recall'].mean(),\n",
    "        'avg_f1': df['f1_score'].mean(),\n",
    "        'avg_answer_quality': df['answer_quality'].mean(),\n",
    "        'avg_citation_quality': df['citation_quality'].mean(),\n",
    "        'avg_num_retrieved': df['num_retrieved'].mean(),\n",
    "        'response_time_std': df['response_time'].std(),\n",
    "        'success_rate': (df['f1_score'] > 0.5).mean()  # Queries with decent F1\n",
    "    }\n",
    "\n",
    "# Analyze each configuration\n",
    "performance_summary = {}\n",
    "for config_name, responses in all_responses.items():\n",
    "    performance = analyze_configuration_performance(responses)\n",
    "    performance_summary[config_name] = performance\n",
    "\n",
    "# Create performance comparison DataFrame\n",
    "df_performance = pd.DataFrame(performance_summary).T\n",
    "print(\"Configuration Performance Comparison:\")\n",
    "print(df_performance.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "configs = list(performance_summary.keys())\n",
    "colors = ['blue', 'green', 'orange']\n",
    "\n",
    "# Response Time\n",
    "response_times = [performance_summary[c]['avg_response_time'] for c in configs]\n",
    "axes[0, 0].bar(range(len(configs)), response_times, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_title('Average Response Time')\n",
    "axes[0, 0].set_ylabel('Seconds')\n",
    "axes[0, 0].set_xticks(range(len(configs)))\n",
    "axes[0, 0].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "\n",
    "# Precision, Recall, F1\n",
    "precision_scores = [performance_summary[c]['avg_precision'] for c in configs]\n",
    "recall_scores = [performance_summary[c]['avg_recall'] for c in configs]\n",
    "f1_scores = [performance_summary[c]['avg_f1'] for c in configs]\n",
    "\n",
    "x = np.arange(len(configs))\n",
    "width = 0.25\n",
    "\n",
    "axes[0, 1].bar(x - width, precision_scores, width, label='Precision', alpha=0.7)\n",
    "axes[0, 1].bar(x, recall_scores, width, label='Recall', alpha=0.7)\n",
    "axes[0, 1].bar(x + width, f1_scores, width, label='F1-Score', alpha=0.7)\n",
    "axes[0, 1].set_title('Retrieval Performance Metrics')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Answer Quality\n",
    "answer_quality = [performance_summary[c]['avg_answer_quality'] for c in configs]\n",
    "axes[0, 2].bar(range(len(configs)), answer_quality, color=colors, alpha=0.7)\n",
    "axes[0, 2].set_title('Average Answer Quality')\n",
    "axes[0, 2].set_ylabel('Quality Score')\n",
    "axes[0, 2].set_xticks(range(len(configs)))\n",
    "axes[0, 2].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "\n",
    "# Success Rate\n",
    "success_rates = [performance_summary[c]['success_rate'] for c in configs]\n",
    "axes[1, 0].bar(range(len(configs)), success_rates, color=colors, alpha=0.7)\n",
    "axes[1, 0].set_title('Success Rate (F1 > 0.5)')\n",
    "axes[1, 0].set_ylabel('Success Rate')\n",
    "axes[1, 0].set_xticks(range(len(configs)))\n",
    "axes[1, 0].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "\n",
    "# Response Time Distribution\n",
    "for i, config in enumerate(configs):\n",
    "    times = [r['response_time'] for r in all_responses[config]]\n",
    "    axes[1, 1].hist(times, alpha=0.5, label=config.split('+')[0].strip(), color=colors[i], bins=10)\n",
    "axes[1, 1].set_title('Response Time Distribution')\n",
    "axes[1, 1].set_xlabel('Response Time (seconds)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Citation Quality\n",
    "citation_quality = [performance_summary[c]['avg_citation_quality'] for c in configs]\n",
    "axes[1, 2].bar(range(len(configs)), citation_quality, color=colors, alpha=0.7)\n",
    "axes[1, 2].set_title('Average Citation Quality')\n",
    "axes[1, 2].set_ylabel('Citation Score')\n",
    "axes[1, 2].set_xticks(range(len(configs)))\n",
    "axes[1, 2].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Difficulty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by query difficulty and category\n",
    "def analyze_by_difficulty(responses, queries):\n",
    "    \"\"\"Analyze performance breakdown by query difficulty\"\"\"\n",
    "    df_responses = pd.DataFrame(responses)\n",
    "    df_queries = pd.DataFrame(queries)\n",
    "    \n",
    "    # Merge responses with query metadata\n",
    "    df_merged = df_responses.merge(df_queries[['query_id', 'difficulty', 'category']], on='query_id')\n",
    "    \n",
    "    # Group by difficulty\n",
    "    difficulty_analysis = df_merged.groupby('difficulty').agg({\n",
    "        'f1_score': ['mean', 'std'],\n",
    "        'answer_quality': ['mean', 'std'],\n",
    "        'response_time': ['mean', 'std'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    # Group by category\n",
    "    category_analysis = df_merged.groupby('category').agg({\n",
    "        'f1_score': ['mean', 'std'],\n",
    "        'answer_quality': ['mean', 'std'],\n",
    "        'response_time': ['mean', 'std'],\n",
    "        'precision': 'mean',\n",
    "        'recall': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    return difficulty_analysis, category_analysis, df_merged\n",
    "\n",
    "# Analyze best performing configuration in detail\n",
    "best_config = df_performance.sort_values('avg_f1', ascending=False).index[0]\n",
    "print(f\"Detailed analysis for best configuration: {best_config}\")\n",
    "\n",
    "difficulty_analysis, category_analysis, df_detailed = analyze_by_difficulty(\n",
    "    all_responses[best_config], \n",
    "    evaluation_queries\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance by Difficulty Level:\")\n",
    "print(difficulty_analysis)\n",
    "\n",
    "print(\"\\nPerformance by Query Category:\")\n",
    "print(category_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive heatmap for performance breakdown\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=[\n",
    "        'F1 Score by Difficulty and Category',\n",
    "        'Answer Quality by Difficulty and Category', \n",
    "        'Response Time by Difficulty',\n",
    "        'Performance Distribution'\n",
    "    ],\n",
    "    specs=[[{'type': 'heatmap'}, {'type': 'heatmap'}],\n",
    "           [{'type': 'box'}, {'type': 'scatter'}]]\n",
    ")\n",
    "\n",
    "# Create pivot tables for heatmaps\n",
    "f1_pivot = df_detailed.pivot_table(\n",
    "    values='f1_score', \n",
    "    index='difficulty', \n",
    "    columns='category', \n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "quality_pivot = df_detailed.pivot_table(\n",
    "    values='answer_quality', \n",
    "    index='difficulty', \n",
    "    columns='category', \n",
    "    aggfunc='mean'\n",
    ").fillna(0)\n",
    "\n",
    "# F1 Score heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=f1_pivot.values,\n",
    "        x=f1_pivot.columns,\n",
    "        y=f1_pivot.index,\n",
    "        colorscale='RdYlBu_r',\n",
    "        showscale=True,\n",
    "        colorbar=dict(x=0.48),\n",
    "        text=f1_pivot.values.round(3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12}\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Answer Quality heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=quality_pivot.values,\n",
    "        x=quality_pivot.columns,\n",
    "        y=quality_pivot.index,\n",
    "        colorscale='RdYlBu_r',\n",
    "        showscale=True,\n",
    "        colorbar=dict(x=1.02),\n",
    "        text=quality_pivot.values.round(3),\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 12}\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Response time by difficulty (box plot)\n",
    "for difficulty in df_detailed['difficulty'].unique():\n",
    "    difficulty_data = df_detailed[df_detailed['difficulty'] == difficulty]\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=difficulty_data['response_time'],\n",
    "            name=difficulty.title(),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# Performance scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_detailed['precision'],\n",
    "        y=df_detailed['recall'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=df_detailed['answer_quality'] * 20,\n",
    "            color=df_detailed['f1_score'],\n",
    "            colorscale='RdYlBu_r',\n",
    "            showscale=True,\n",
    "            colorbar=dict(x=1.02, y=0.2)\n",
    "        ),\n",
    "        text=df_detailed['query_id'],\n",
    "        hovertemplate='<b>%{text}</b><br>Precision: %{x:.3f}<br>Recall: %{y:.3f}<extra></extra>',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=f\"Performance Analysis: {best_config}\",\n",
    "    title_x=0.5,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_yaxes(title_text=\"Response Time (s)\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Precision\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Recall\", row=2, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis and Failure Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and analyze failure cases\n",
    "def identify_failure_cases(responses, queries, threshold=0.3):\n",
    "    \"\"\"Identify queries with poor performance\"\"\"\n",
    "    df_responses = pd.DataFrame(responses)\n",
    "    df_queries = pd.DataFrame(queries)\n",
    "    df_merged = df_responses.merge(df_queries, on='query_id')\n",
    "    \n",
    "    # Define failure cases\n",
    "    failures = df_merged[\n",
    "        (df_merged['f1_score'] < threshold) |\n",
    "        (df_merged['answer_quality'] < threshold) |\n",
    "        (df_merged['precision'] < threshold)\n",
    "    ]\n",
    "    \n",
    "    return failures\n",
    "\n",
    "# Analyze failure patterns\n",
    "failure_analysis = {}\n",
    "\n",
    "for config_name, responses in all_responses.items():\n",
    "    failures = identify_failure_cases(responses, evaluation_queries)\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        failure_patterns = {\n",
    "            'total_failures': len(failures),\n",
    "            'failure_rate': len(failures) / len(responses),\n",
    "            'by_difficulty': failures['difficulty'].value_counts().to_dict(),\n",
    "            'by_category': failures['category'].value_counts().to_dict(),\n",
    "            'avg_f1_failures': failures['f1_score'].mean(),\n",
    "            'avg_quality_failures': failures['answer_quality'].mean(),\n",
    "            'common_issues': []\n",
    "        }\n",
    "        \n",
    "        # Identify common failure patterns\n",
    "        low_precision = failures[failures['precision'] < 0.3]\n",
    "        low_recall = failures[failures['recall'] < 0.3]\n",
    "        low_quality = failures[failures['answer_quality'] < 0.3]\n",
    "        \n",
    "        if len(low_precision) > 0:\n",
    "            failure_patterns['common_issues'].append(f\"Low precision ({len(low_precision)} queries)\")\n",
    "        if len(low_recall) > 0:\n",
    "            failure_patterns['common_issues'].append(f\"Low recall ({len(low_recall)} queries)\")\n",
    "        if len(low_quality) > 0:\n",
    "            failure_patterns['common_issues'].append(f\"Low answer quality ({len(low_quality)} queries)\")\n",
    "        \n",
    "        failure_analysis[config_name] = failure_patterns\n",
    "    \n",
    "    print(f\"\\n{config_name} - Failure Analysis:\")\n",
    "    print(f\"  Total failures: {len(failures)} / {len(responses)} ({len(failures)/len(responses)*100:.1f}%)\")\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        print(f\"  Failure by difficulty: {failures['difficulty'].value_counts().to_dict()}\")\n",
    "        print(f\"  Failure by category: {failures['category'].value_counts().to_dict()}\")\n",
    "        print(f\"  Failed queries: {failures['query_id'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize failure analysis\n",
    "if failure_analysis:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Failure rates by configuration\n",
    "    configs = list(failure_analysis.keys())\n",
    "    failure_rates = [failure_analysis[c]['failure_rate'] * 100 for c in configs]\n",
    "    \n",
    "    axes[0, 0].bar(range(len(configs)), failure_rates, alpha=0.7, color='red')\n",
    "    axes[0, 0].set_title('Failure Rate by Configuration')\n",
    "    axes[0, 0].set_ylabel('Failure Rate (%)')\n",
    "    axes[0, 0].set_xticks(range(len(configs)))\n",
    "    axes[0, 0].set_xticklabels([c.split('+')[0].strip() for c in configs], rotation=45)\n",
    "    \n",
    "    # Failure distribution by difficulty (best config)\n",
    "    if configs:\n",
    "        best_config_failures = failure_analysis[configs[0]]\n",
    "        if best_config_failures['by_difficulty']:\n",
    "            difficulties = list(best_config_failures['by_difficulty'].keys())\n",
    "            counts = list(best_config_failures['by_difficulty'].values())\n",
    "            axes[0, 1].pie(counts, labels=difficulties, autopct='%1.1f%%')\n",
    "            axes[0, 1].set_title('Failure Distribution by Difficulty')\n",
    "    \n",
    "    # Performance comparison: success vs failure cases\n",
    "    best_config_name = list(all_responses.keys())[0]\n",
    "    all_responses_df = pd.DataFrame(all_responses[best_config_name])\n",
    "    failures_df = identify_failure_cases(all_responses[best_config_name], evaluation_queries)\n",
    "    successes_df = all_responses_df[~all_responses_df['query_id'].isin(failures_df['query_id'])]\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1_score', 'answer_quality']\n",
    "    success_means = [successes_df[m].mean() for m in metrics]\n",
    "    failure_means = [failures_df[m].mean() for m in metrics] if len(failures_df) > 0 else [0] * len(metrics)\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, success_means, width, label='Success Cases', alpha=0.7, color='green')\n",
    "    axes[1, 0].bar(x + width/2, failure_means, width, label='Failure Cases', alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('Success vs Failure Cases')\n",
    "    axes[1, 0].set_ylabel('Average Score')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(metrics, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Response time distribution for success vs failure\n",
    "    if len(successes_df) > 0:\n",
    "        axes[1, 1].hist(successes_df['response_time'], alpha=0.5, label='Success', color='green', bins=8)\n",
    "    if len(failures_df) > 0:\n",
    "        axes[1, 1].hist(failures_df['response_time'], alpha=0.5, label='Failure', color='red', bins=8)\n",
    "    axes[1, 1].set_title('Response Time: Success vs Failure')\n",
    "    axes[1, 1].set_xlabel('Response Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\nelse:\n",
    "    print(\"No significant failures detected across configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization recommendations based on analysis\n",
    "def generate_recommendations(performance_summary, failure_analysis):\n",
    "    \"\"\"Generate optimization recommendations based on evaluation results\"\"\"\n",
    "    \n",
    "    # Find best and worst performing configurations\n",
    "    df_perf = pd.DataFrame(performance_summary).T\n",
    "    best_config = df_perf.sort_values('avg_f1', ascending=False).index[0]\n",
    "    worst_config = df_perf.sort_values('avg_f1', ascending=True).index[0]\n",
    "    \n",
    "    recommendations = {\n",
    "        'model_selection': {\n",
    "            'best_overall': best_config,\n",
    "            'best_speed': df_perf.sort_values('avg_response_time', ascending=True).index[0],\n",
    "            'best_quality': df_perf.sort_values('avg_answer_quality', ascending=False).index[0]\n",
    "        },\n",
    "        'performance_improvements': [],\n",
    "        'cost_optimizations': [],\n",
    "        'quality_enhancements': [],\n",
    "        'technical_recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Performance improvements\n",
    "    avg_response_time = df_perf['avg_response_time'].mean()\n",
    "    if avg_response_time > 3.0:\n",
    "        recommendations['performance_improvements'].append(\n",
    "            \"Consider caching embeddings and implementing batch processing to reduce response times\"\n",
    "        )\n",
    "    \n",
    "    if df_perf['avg_recall'].mean() < 0.6:\n",
    "        recommendations['performance_improvements'].append(\n",
    "            \"Increase retrieval K value or improve chunking strategy to enhance recall\"\n",
    "        )\n",
    "    \n",
    "    if df_perf['avg_precision'].mean() < 0.7:\n",
    "        recommendations['performance_improvements'].append(\n",
    "            \"Fine-tune similarity thresholds or implement re-ranking to improve precision\"\n",
    "        )\n",
    "    \n",
    "    # Cost optimizations\n",
    "    if 'OpenAI' in best_config:\n",
    "        recommendations['cost_optimizations'].append(\n",
    "            \"Consider hybrid approach: use local embeddings for development and OpenAI for production\"\n",
    "        )\n",
    "    \n",
    "    recommendations['cost_optimizations'].extend([\n",
    "        \"Implement embedding caching to reduce API calls\",\n",
    "        \"Use batch processing for multiple queries\",\n",
    "        \"Monitor token usage and implement usage-based optimization\"\n",
    "    ])\n",
    "    \n",
    "    # Quality enhancements\n",
    "    if failure_analysis:\n",
    "        common_failure_patterns = []\n",
    "        for config, analysis in failure_analysis.items():\n",
    "            if analysis['failure_rate'] > 0.2:\n",
    "                common_failure_patterns.extend(analysis.get('common_issues', []))\n",
    "        \n",
    "        if 'Low precision' in str(common_failure_patterns):\n",
    "            recommendations['quality_enhancements'].append(\n",
    "                \"Implement semantic re-ranking or filtering to improve precision\"\n",
    "            )\n",
    "        \n",
    "        if 'Low recall' in str(common_failure_patterns):\n",
    "            recommendations['quality_enhancements'].append(\n",
    "                \"Expand query processing with synonyms and related terms\"\n",
    "            )\n",
    "    \n",
    "    recommendations['quality_enhancements'].extend([\n",
    "        \"Implement query expansion and reformulation\",\n",
    "        \"Add metadata filtering for domain-specific queries\",\n",
    "        \"Use ensemble retrieval combining multiple strategies\"\n",
    "    ])\n",
    "    \n",
    "    # Technical recommendations\n",
    "    recommendations['technical_recommendations'].extend([\n",
    "        \"Set up A/B testing framework for model comparisons\",\n",
    "        \"Implement real-time monitoring for performance metrics\",\n",
    "        \"Create evaluation pipeline with human feedback integration\",\n",
    "        \"Establish baseline metrics and regression testing\",\n",
    "        \"Deploy gradual rollout strategy for model updates\"\n",
    "    ])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_recommendations(performance_summary, failure_analysis)\n",
    "\n",
    "print(\"🎯 RAG System Optimization Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n🏆 MODEL SELECTION:\")\n",
    "print(f\"  Best Overall: {recommendations['model_selection']['best_overall']}\")\n",
    "print(f\"  Best Speed: {recommendations['model_selection']['best_speed']}\")\n",
    "print(f\"  Best Quality: {recommendations['model_selection']['best_quality']}\")\n",
    "\n",
    "print(\"\\n⚡ PERFORMANCE IMPROVEMENTS:\")\n",
    "for i, rec in enumerate(recommendations['performance_improvements'], 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n💰 COST OPTIMIZATIONS:\")\n",
    "for i, rec in enumerate(recommendations['cost_optimizations'], 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n🔧 QUALITY ENHANCEMENTS:\")\n",
    "for i, rec in enumerate(recommendations['quality_enhancements'], 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n🛠️ TECHNICAL RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(recommendations['technical_recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Roadmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create implementation roadmap\n",
    "roadmap = {\n",
    "    \"Phase 1: Foundation (Week 1-2)\": [\n",
    "        \"Implement best-performing configuration from evaluation\",\n",
    "        \"Set up embedding caching system\",\n",
    "        \"Create basic monitoring and logging\",\n",
    "        \"Deploy initial version with selected model\"\n",
    "    ],\n",
    "    \"Phase 2: Optimization (Week 3-4)\": [\n",
    "        \"Implement query processing improvements\",\n",
    "        \"Add retrieval re-ranking mechanism\",\n",
    "        \"Optimize chunking strategy based on evaluation\",\n",
    "        \"Set up A/B testing framework\"\n",
    "    ],\n",
    "    \"Phase 3: Enhancement (Week 5-6)\": [\n",
    "        \"Add metadata filtering and faceted search\",\n",
    "        \"Implement ensemble retrieval methods\",\n",
    "        \"Create user feedback collection system\",\n",
    "        \"Develop evaluation automation\"\n",
    "    ],\n",
    "    \"Phase 4: Scale & Monitor (Week 7-8)\": [\n",
    "        \"Implement production monitoring dashboard\",\n",
    "        \"Set up automated performance regression testing\",\n",
    "        \"Create cost monitoring and alerting\",\n",
    "        \"Establish continuous evaluation pipeline\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"🗺️ RAG System Implementation Roadmap\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for phase, tasks in roadmap.items():\n",
    "    print(f\"\\n{phase}:\")\n",
    "    for i, task in enumerate(tasks, 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "\n",
    "# Create success metrics\n",
    "success_metrics = {\n",
    "    \"Performance Targets\": {\n",
    "        \"Average F1 Score\": \"> 0.7\",\n",
    "        \"Average Response Time\": \"< 2.0 seconds\",\n",
    "        \"Success Rate\": \"> 85%\",\n",
    "        \"User Satisfaction\": \"> 4.0/5.0\"\n",
    "    },\n",
    "    \"Quality Targets\": {\n",
    "        \"Answer Relevance\": \"> 0.8\",\n",
    "        \"Citation Accuracy\": \"> 0.9\",\n",
    "        \"Factual Correctness\": \"> 0.85\",\n",
    "        \"Coherence Score\": \"> 0.8\"\n",
    "    },\n",
    "    \"Operational Targets\": {\n",
    "        \"System Uptime\": \"> 99.5%\",\n",
    "        \"Error Rate\": \"< 1%\",\n",
    "        \"Cost per Query\": \"< $0.05\",\n",
    "        \"Monthly Active Users\": \"> 1000\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\\n📊 Success Metrics & Targets\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for category, metrics in success_metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric, target in metrics.items():\n",
    "        print(f\"  • {metric}: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive evaluation results\n",
    "final_evaluation_results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'evaluation_summary': {\n",
    "        'total_queries': len(evaluation_queries),\n",
    "        'configurations_tested': len(model_configurations),\n",
    "        'best_configuration': recommendations['model_selection']['best_overall']\n",
    "    },\n",
    "    'performance_metrics': performance_summary,\n",
    "    'failure_analysis': failure_analysis,\n",
    "    'recommendations': recommendations,\n",
    "    'implementation_roadmap': roadmap,\n",
    "    'success_metrics': success_metrics,\n",
    "    'detailed_results': {\n",
    "        'evaluation_queries': evaluation_queries,\n",
    "        'all_responses': all_responses\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "output_file = processed_dir / 'evaluation_analysis_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(final_evaluation_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✅ Comprehensive evaluation results saved to {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "RAG System Evaluation Summary Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "{'='*50}\n",
    "\n",
    "EVALUATION OVERVIEW:\n",
    "• Queries Tested: {len(evaluation_queries)}\n",
    "• Configurations: {len(model_configurations)}\n",
    "• Best Overall: {recommendations['model_selection']['best_overall']}\n",
    "\n",
    "KEY FINDINGS:\n",
    "• Average F1 Score: {df_performance['avg_f1'].max():.3f}\n",
    "• Average Response Time: {df_performance['avg_response_time'].min():.2f}s\n",
    "• Success Rate: {df_performance['success_rate'].max():.1%}\n",
    "\n",
    "TOP RECOMMENDATIONS:\n",
    "1. Deploy {recommendations['model_selection']['best_overall']} as primary configuration\n",
    "2. Implement embedding caching for cost optimization\n",
    "3. Set up A/B testing framework for continuous improvement\n",
    "4. Create monitoring dashboard for production deployment\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Review implementation roadmap\n",
    "2. Begin Phase 1 deployment\n",
    "3. Set up monitoring and evaluation pipeline\n",
    "4. Collect user feedback for continuous improvement\n",
    "\"\"\"\n",
    "\n",
    "print(summary_report)\n",
    "\n",
    "# Save summary report\n",
    "with open(processed_dir / 'evaluation_summary_report.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n🎉 Evaluation analysis completed successfully!\")\n",
    "print(\"📋 Summary report saved for stakeholder review.\")\n",
    "print(\"🚀 Ready for production deployment planning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}