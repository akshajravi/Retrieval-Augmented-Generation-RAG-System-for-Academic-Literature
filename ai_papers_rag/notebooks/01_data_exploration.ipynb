{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Papers RAG - Data Exploration\n",
    "\n",
    "This notebook explores the research papers dataset and analyzes document characteristics for the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "data_dir = Path().parent / \"data\"\n",
    "raw_papers_dir = data_dir / \"raw_papers\"\n",
    "processed_dir = data_dir / \"processed\"\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Raw papers directory: {raw_papers_dir}\")\n",
    "print(f\"Processed directory: {processed_dir}\")\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"\\nDirectory status:\")\n",
    "print(f\"Raw papers exists: {raw_papers_dir.exists()}\")\n",
    "print(f\"Processed exists: {processed_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan for PDF files\n",
    "if raw_papers_dir.exists():\n",
    "    pdf_files = list(raw_papers_dir.rglob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    if pdf_files:\n",
    "        # Display first few files\n",
    "        print(\"\\nFirst 10 files:\")\n",
    "        for i, file in enumerate(pdf_files[:10], 1):\n",
    "            file_size = file.stat().st_size / (1024 * 1024)  # MB\n",
    "            print(f\"{i:2d}. {file.name} ({file_size:.2f} MB)\")\n",
    "    else:\n",
    "        print(\"No PDF files found. Please add some research papers to the raw_papers directory.\")\nelse:\n",
    "    print(\"Raw papers directory not found. Please run the setup script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document characteristics (when PDFs are available)\n",
    "if raw_papers_dir.exists() and pdf_files:\n",
    "    # Create a dataset of file metadata\n",
    "    file_data = []\n",
    "    \n",
    "    for pdf_file in pdf_files[:20]:  # Limit to first 20 for demonstration\n",
    "        try:\n",
    "            stat = pdf_file.stat()\n",
    "            file_info = {\n",
    "                'filename': pdf_file.name,\n",
    "                'size_mb': stat.st_size / (1024 * 1024),\n",
    "                'created': pd.to_datetime(stat.st_ctime, unit='s'),\n",
    "                'modified': pd.to_datetime(stat.st_mtime, unit='s'),\n",
    "                'extension': pdf_file.suffix.lower()\n",
    "            }\n",
    "            file_data.append(file_info)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_file.name}: {e}\")\n",
    "    \n",
    "    if file_data:\n",
    "        df_files = pd.DataFrame(file_data)\n",
    "        print(f\"Dataset created with {len(df_files)} files\")\n",
    "        print(\"\\nDataset overview:\")\n",
    "        print(df_files.head())\n",
    "        \n",
    "        print(\"\\nFile size statistics:\")\n",
    "        print(df_files['size_mb'].describe())\nelse:\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data when no PDFs are available\n",
    "    np.random.seed(42)\n",
    "    sample_papers = [\n",
    "        \"attention_is_all_you_need.pdf\",\n",
    "        \"bert_pretraining_transformers.pdf\",\n",
    "        \"gpt3_language_models.pdf\",\n",
    "        \"transformer_xl_context.pdf\",\n",
    "        \"roberta_optimized_pretraining.pdf\",\n",
    "        \"t5_text_to_text.pdf\",\n",
    "        \"electra_efficient_pretraining.pdf\",\n",
    "        \"deberta_improved_bert.pdf\",\n",
    "        \"switch_transformer_scaling.pdf\",\n",
    "        \"palm_pathways_language.pdf\"\n",
    "    ]\n",
    "    \n",
    "    file_data = []\n",
    "    for i, filename in enumerate(sample_papers):\n",
    "        file_info = {\n",
    "            'filename': filename,\n",
    "            'size_mb': np.random.normal(2.5, 1.0),  # Average 2.5MB\n",
    "            'created': pd.Timestamp.now() - pd.Timedelta(days=np.random.randint(1, 365)),\n",
    "            'pages': np.random.randint(8, 25),\n",
    "            'topic': np.random.choice(['transformers', 'bert', 'gpt', 'language_models', 'attention']),\n",
    "            'year': np.random.randint(2017, 2024)\n",
    "        }\n",
    "        file_data.append(file_info)\n",
    "    \n",
    "    df_files = pd.DataFrame(file_data)\n",
    "    print(f\"Sample dataset created with {len(df_files)} files\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df_files.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File size distribution\n",
    "if 'df_files' in locals() and not df_files.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # File size histogram\n",
    "    axes[0, 0].hist(df_files['size_mb'], bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of File Sizes')\n",
    "    axes[0, 0].set_xlabel('Size (MB)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Pages distribution (if available)\n",
    "    if 'pages' in df_files.columns:\n",
    "        axes[0, 1].hist(df_files['pages'], bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[0, 1].set_title('Distribution of Page Counts')\n",
    "        axes[0, 1].set_xlabel('Number of Pages')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Topic distribution (if available)\n",
    "    if 'topic' in df_files.columns:\n",
    "        topic_counts = df_files['topic'].value_counts()\n",
    "        axes[1, 0].bar(topic_counts.index, topic_counts.values, alpha=0.7, color='lightgreen')\n",
    "        axes[1, 0].set_title('Papers by Topic')\n",
    "        axes[1, 0].set_xlabel('Topic')\n",
    "        axes[1, 0].set_ylabel('Count')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Year distribution (if available)\n",
    "    if 'year' in df_files.columns:\n",
    "        year_counts = df_files['year'].value_counts().sort_index()\n",
    "        axes[1, 1].plot(year_counts.index, year_counts.values, marker='o', color='purple', linewidth=2)\n",
    "        axes[1, 1].set_title('Papers by Publication Year')\n",
    "        axes[1, 1].set_xlabel('Year')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualizations with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot of file characteristics\n",
    "if 'df_files' in locals() and not df_files.empty:\n",
    "    if 'pages' in df_files.columns and 'year' in df_files.columns:\n",
    "        fig = px.scatter(\n",
    "            df_files, \n",
    "            x='pages', \n",
    "            y='size_mb',\n",
    "            color='topic' if 'topic' in df_files.columns else None,\n",
    "            size='year' if 'year' in df_files.columns else None,\n",
    "            hover_data=['filename'],\n",
    "            title='Paper Characteristics: Pages vs File Size',\n",
    "            labels={\n",
    "                'pages': 'Number of Pages',\n",
    "                'size_mb': 'File Size (MB)',\n",
    "                'topic': 'Research Topic'\n",
    "            }\n",
    "        )\n",
    "        fig.update_traces(marker=dict(sizemode='diameter', sizeref=1))\n",
    "        fig.show()\n",
    "    else:\n",
    "        # Simple bar chart of file sizes\n",
    "        fig = px.bar(\n",
    "            df_files.head(10), \n",
    "            x='filename', \n",
    "            y='size_mb',\n",
    "            title='File Sizes of Sample Papers',\n",
    "            labels={'filename': 'Paper', 'size_mb': 'Size (MB)'}\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.show()\nelse:\n",
    "    print(\"No data available for interactive visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate text extraction and analysis\n",
    "print(\"Text Analysis Preview (Simulated Data)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample text analysis results\n",
    "sample_text_stats = {\n",
    "    'avg_words_per_page': 450,\n",
    "    'avg_sentences_per_page': 28,\n",
    "    'avg_paragraphs_per_page': 12,\n",
    "    'common_keywords': ['transformer', 'attention', 'neural', 'language', 'model', \n",
    "                       'training', 'performance', 'architecture', 'learning', 'deep'],\n",
    "    'avg_chars_per_word': 5.2,\n",
    "    'readability_score': 0.65  # Flesch Reading Ease equivalent\n",
    "}\n",
    "\n",
    "print(f\"Average words per page: {sample_text_stats['avg_words_per_page']}\")\n",
    "print(f\"Average sentences per page: {sample_text_stats['avg_sentences_per_page']}\")\n",
    "print(f\"Average paragraphs per page: {sample_text_stats['avg_paragraphs_per_page']}\")\n",
    "print(f\"Average characters per word: {sample_text_stats['avg_chars_per_word']}\")\n",
    "print(f\"Readability score: {sample_text_stats['readability_score']:.2f}\")\n",
    "\n",
    "print(\"\\nMost common keywords:\")\n",
    "for i, keyword in enumerate(sample_text_stats['common_keywords'][:10], 1):\n",
    "    print(f\"{i:2d}. {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimal chunking strategies\n",
    "print(\"Chunking Strategy Analysis\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Simulate different chunking approaches\n",
    "chunk_strategies = {\n",
    "    'Fixed Size (500 words)': {\n",
    "        'avg_chunks_per_paper': 18,\n",
    "        'avg_chunk_length': 500,\n",
    "        'overlap_efficiency': 0.85,\n",
    "        'semantic_coherence': 0.72\n",
    "    },\n",
    "    'Fixed Size (1000 words)': {\n",
    "        'avg_chunks_per_paper': 9,\n",
    "        'avg_chunk_length': 1000,\n",
    "        'overlap_efficiency': 0.88,\n",
    "        'semantic_coherence': 0.78\n",
    "    },\n",
    "    'Paragraph-based': {\n",
    "        'avg_chunks_per_paper': 24,\n",
    "        'avg_chunk_length': 380,\n",
    "        'overlap_efficiency': 0.82,\n",
    "        'semantic_coherence': 0.85\n",
    "    },\n",
    "    'Section-based': {\n",
    "        'avg_chunks_per_paper': 6,\n",
    "        'avg_chunk_length': 1500,\n",
    "        'overlap_efficiency': 0.90,\n",
    "        'semantic_coherence': 0.92\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "df_chunks = pd.DataFrame(chunk_strategies).T\n",
    "print(\"Chunking Strategy Comparison:\")\n",
    "print(df_chunks.round(2))\n",
    "\n",
    "# Visualize chunking strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Chunks per paper vs semantic coherence\n",
    "axes[0].scatter(df_chunks['avg_chunks_per_paper'], df_chunks['semantic_coherence'], \n",
    "               s=100, alpha=0.7, c=['red', 'blue', 'green', 'orange'])\n",
    "for i, strategy in enumerate(df_chunks.index):\n",
    "    axes[0].annotate(strategy, \n",
    "                    (df_chunks.loc[strategy, 'avg_chunks_per_paper'], \n",
    "                     df_chunks.loc[strategy, 'semantic_coherence']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "axes[0].set_xlabel('Average Chunks per Paper')\n",
    "axes[0].set_ylabel('Semantic Coherence Score')\n",
    "axes[0].set_title('Chunking Strategy Trade-offs')\n",
    "\n",
    "# Average chunk length distribution\n",
    "strategies = list(chunk_strategies.keys())\n",
    "chunk_lengths = [chunk_strategies[s]['avg_chunk_length'] for s in strategies]\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "axes[1].bar(range(len(strategies)), chunk_lengths, color=colors, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(strategies)))\n",
    "axes[1].set_xticklabels([s.split('(')[0].strip() for s in strategies], rotation=45)\n",
    "axes[1].set_ylabel('Average Chunk Length (words)')\n",
    "axes[1].set_title('Chunk Length by Strategy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Data Exploration Summary and Recommendations\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "recommendations = [\n",
    "    \"🎯 **Chunking Strategy**: Based on analysis, section-based chunking provides the best semantic coherence\",\n",
    "    \"📊 **Optimal Chunk Size**: 1000-1500 words balances context and specificity\",\n",
    "    \"🔄 **Overlap**: 200-word overlap recommended for maintaining context across chunks\",\n",
    "    \"📚 **Dataset Size**: Current dataset suitable for initial testing and development\",\n",
    "    \"🏷️ **Metadata**: Extract paper titles, authors, publication dates for better filtering\",\n",
    "    \"🔍 **Keywords**: Focus on technical terms like 'transformer', 'attention', 'neural' for indexing\",\n",
    "    \"⚡ **Performance**: Expect ~9-24 chunks per paper depending on strategy chosen\",\n",
    "    \"🎨 **Visualization**: Use interactive plots for exploring large document collections\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Run the document ingestion pipeline\")\n",
    "print(\"2. Proceed to embedding experiments (notebook 02)\")\n",
    "print(\"3. Test different chunking strategies with real data\")\n",
    "print(\"4. Monitor retrieval quality in production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results for later use\n",
    "analysis_results = {\n",
    "    'dataset_stats': df_files.describe().to_dict() if 'df_files' in locals() else {},\n",
    "    'recommended_chunk_size': 1000,\n",
    "    'recommended_overlap': 200,\n",
    "    'chunking_strategies': chunk_strategies,\n",
    "    'text_stats': sample_text_stats\n",
    "}\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as JSON for easy loading in other notebooks\n",
    "import json\n",
    "with open(processed_dir / 'data_exploration_results.json', 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ Analysis results saved to processed/data_exploration_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}